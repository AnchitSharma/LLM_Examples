{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYmYweYmi2HJooEHrR4U7/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnchitSharma/LLM_Examples/blob/main/Langchain_cookbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ekVUP_N9cnJh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'YOUR_API_KEY'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "8zIq0thseMvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "bZZ7GZSqeWRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building A Language Model Application\n",
        "\n",
        "## LLMS: Get predictions from a language model"
      ],
      "metadata": {
        "id": "n7Qtxs8-egy9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "WTzSyZDQd1qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "D9suBtuveJe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"What are 5 vacation destination for someone who likes to eat pasta?\"\n",
        "print(llm(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCEuJs46e20f",
        "outputId": "4fc996fa-f03b-4365-c0c0-202c2842a7d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Rome, Italy\n",
            "2. Sorrento, Italy\n",
            "3. Florence, Italy\n",
            "4. Bologna, Italy\n",
            "5. Sicily, Italy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prompt Templates: Manage prompts for LLMs**"
      ],
      "metadata": {
        "id": "YsBrDMjvhGy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "33zUs7ZkfRVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"food\"],\n",
        "    template = \"What are 5 vacation destinations for someone who likes to eat {food}?\"\n",
        ")"
      ],
      "metadata": {
        "id": "JtUhENjyfE5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.format(food=\"dessert\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PAVpX_6Kh858",
        "outputId": "48d07514-26f2-43b6-e95f-5173596e17a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What are 5 vacation destinations for someone who likes to eat dessert?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm(prompt.format(food=\"dessert\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAq57ZHEiE1V",
        "outputId": "e069a840-de28-4704-ab24-17cf3456cd6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Paris, France\n",
            "2. Rome, Italy\n",
            "3. London, England \n",
            "4. Tokyo, Japan \n",
            "5. Honolulu, Hawaii\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chains: Combine LLMs and prompts in multi-step workflows**"
      ],
      "metadata": {
        "id": "9q5vBTTgipop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "8LepeGT0iMbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0.9)"
      ],
      "metadata": {
        "id": "lMzMe-51jEkV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompts = PromptTemplate(\n",
        "    input_variables = [\"food\"],\n",
        "    template=\"What are 5 vacation destination who likes to eat {food}?\"\n",
        ")"
      ],
      "metadata": {
        "id": "I8bpPVJPjU75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt)"
      ],
      "metadata": {
        "id": "aL_6s2c1jjBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.run(\"fruit\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmqRdL7ejptv",
        "outputId": "8d9bb8a6-ebad-479e-f26e-5bce6c3f4c43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. Hawaii \n",
            "2. Costa Rica \n",
            "3. Jamaica \n",
            "4. Thailand \n",
            "5. Philippines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-search-results"
      ],
      "metadata": {
        "id": "id63xFJdjudt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import load_tools\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "6gA6aysTliK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['SERPAPI_API_KEY'] = 'YOUR_API_KEY'"
      ],
      "metadata": {
        "id": "FGcWX-OSlzQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "\n",
        "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
      ],
      "metadata": {
        "id": "pqk4UJril8RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent = initialize_agent(tools, llm, agent='zero-shot-react-description', verbose=True)"
      ],
      "metadata": {
        "id": "Y3uTVtTWmaXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent.run(\"Who is the current leader of Israel? What is the largest prime number that is smaller than their age?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "3nS_p9qPmsZO",
        "outputId": "98fad781-f67a-4136-af79-16e17200aff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            "\u001b[32;1m\u001b[1;3m I need to find out who the leader of Israel is and then use a calculator to find the largest prime number that is smaller than their age.\n",
            "Action: Search\n",
            "Action Input: \"current leader of Israel\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mBenjamin Netanyahu\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I need to find out how old Benjamin Netanyahu is.\n",
            "Action: Search\n",
            "Action Input: \"age of Benjamin Netanyahu\"\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3m73 years\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-0Vz7WWVe5aeFli6azXqgr78g on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I need to use a calculator to find the largest prime number that is smaller than 73.\n",
            "Action: Calculator\n",
            "Action Input: 73\u001b[0m"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-0Vz7WWVe5aeFli6azXqgr78g on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-0Vz7WWVe5aeFli6azXqgr78g on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-0Vz7WWVe5aeFli6azXqgr78g on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Observation: \u001b[33;1m\u001b[1;3mAnswer: 73\u001b[0m\n",
            "Thought:"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-0Vz7WWVe5aeFli6azXqgr78g on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-0Vz7WWVe5aeFli6azXqgr78g on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
            "WARNING:langchain.llms.base:Retrying langchain.llms.openai.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for text-davinci-003 in organization org-0Vz7WWVe5aeFli6azXqgr78g on requests per min. Limit: 3 / min. Please try again in 20s. Contact us through our help center at help.openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
            "Final Answer: The current leader of Israel is Benjamin Netanyahu and the largest prime number that is smaller than their age is 73.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The current leader of Israel is Benjamin Netanyahu and the largest prime number that is smaller than their age is 73.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import OpenAI, ConversationChain"
      ],
      "metadata": {
        "id": "0AvfNciKngTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = OpenAI(temperature=0)\n",
        "conversation = ConversationChain(llm=llm, verbose=True)"
      ],
      "metadata": {
        "id": "51kAzb6ypDye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"you are an expert into Converting Mathematical Model to GAMS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "pimqGfjhpPY0",
        "outputId": "c9a4c669-eab3-4f25-8921-2e1eb08922d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: you are an expert into Converting Mathematical Model to GAMS\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Yes, I am an expert in converting mathematical models to GAMS. I have been working with GAMS for many years and have a deep understanding of the language and its capabilities. I can help you convert your mathematical models into GAMS code quickly and accurately.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"Can you show an example of how to convert a mathematical model into GAMS\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "5LdJ3IrOpXHf",
        "outputId": "8b3d2915-0e63-434b-a63b-9f8ad6df6df9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: you are an expert into Converting Mathematical Model to GAMS\n",
            "AI:  Yes, I am an expert in converting mathematical models to GAMS. I have been working with GAMS for many years and have a deep understanding of the language and its capabilities. I can help you convert your mathematical models into GAMS code quickly and accurately.\n",
            "Human: Can you show an example of how to convert a mathematical model into GAMS\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\" Sure! Here is an example of how to convert a mathematical model into GAMS: \\n\\nLet's say you have a mathematical model that looks like this:\\n\\nMaximize Z = 3x + 4y\\nSubject to:\\n2x + y <= 10\\nx + 2y <= 8\\nx, y >= 0\\n\\nThis can be converted into GAMS code as follows:\\n\\nVariables\\nx 'x variable'\\ny 'y variable'\\n\\nEquations\\nobj 'Objective Function'\\ncon1 'Constraint 1'\\ncon2 'Constraint 2'\\n\\nobj.. Z =e= 3*x + 4*y;\\ncon1.. 2*x + y =l= 10;\\ncon2.. x + 2*y =l= 8;\\n\\nx.lo = 0;\\ny.lo = 0;\\n\\nSolve obj using lp maximizing Z;\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What was the first thing I said to you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 802
        },
        "id": "umwCIgG7puNp",
        "outputId": "391a1c32-8e23-48be-b000-1b53840d6229"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: you are an expert into Converting Mathematical Model to GAMS\n",
            "AI:  Yes, I am an expert in converting mathematical models to GAMS. I have been working with GAMS for many years and have a deep understanding of the language and its capabilities. I can help you convert your mathematical models into GAMS code quickly and accurately.\n",
            "Human: Can you show an example of how to convert a mathematical model into GAMS\n",
            "AI:  Sure! Here is an example of how to convert a mathematical model into GAMS: \n",
            "\n",
            "Let's say you have a mathematical model that looks like this:\n",
            "\n",
            "Maximize Z = 3x + 4y\n",
            "Subject to:\n",
            "2x + y <= 10\n",
            "x + 2y <= 8\n",
            "x, y >= 0\n",
            "\n",
            "This can be converted into GAMS code as follows:\n",
            "\n",
            "Variables\n",
            "x 'x variable'\n",
            "y 'y variable'\n",
            "\n",
            "Equations\n",
            "obj 'Objective Function'\n",
            "con1 'Constraint 1'\n",
            "con2 'Constraint 2'\n",
            "\n",
            "obj.. Z =e= 3*x + 4*y;\n",
            "con1.. 2*x + y =l= 10;\n",
            "con2.. x + 2*y =l= 8;\n",
            "\n",
            "x.lo = 0;\n",
            "y.lo = 0;\n",
            "\n",
            "Solve obj using lp maximizing Z;\n",
            "Human: What was the first thing I said to you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' You asked me if I was an expert in converting mathematical models to GAMS.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversation.predict(input=\"What is an alternative phrase for the first thing I said to you?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "BOvCwtmsqBfU",
        "outputId": "53337449-e83d-454a-ec9d-225142c5700d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "Human: you are an expert into Converting Mathematical Model to GAMS\n",
            "AI:  Yes, I am an expert in converting mathematical models to GAMS. I have been working with GAMS for many years and have a deep understanding of the language and its capabilities. I can help you convert your mathematical models into GAMS code quickly and accurately.\n",
            "Human: Can you show an example of how to convert a mathematical model into GAMS\n",
            "AI:  Sure! Here is an example of how to convert a mathematical model into GAMS: \n",
            "\n",
            "Let's say you have a mathematical model that looks like this:\n",
            "\n",
            "Maximize Z = 3x + 4y\n",
            "Subject to:\n",
            "2x + y <= 10\n",
            "x + 2y <= 8\n",
            "x, y >= 0\n",
            "\n",
            "This can be converted into GAMS code as follows:\n",
            "\n",
            "Variables\n",
            "x 'x variable'\n",
            "y 'y variable'\n",
            "\n",
            "Equations\n",
            "obj 'Objective Function'\n",
            "con1 'Constraint 1'\n",
            "con2 'Constraint 2'\n",
            "\n",
            "obj.. Z =e= 3*x + 4*y;\n",
            "con1.. 2*x + y =l= 10;\n",
            "con2.. x + 2*y =l= 8;\n",
            "\n",
            "x.lo = 0;\n",
            "y.lo = 0;\n",
            "\n",
            "Solve obj using lp maximizing Z;\n",
            "Human: What was the first thing I said to you?\n",
            "AI:  You asked me if I was an expert in converting mathematical models to GAMS.\n",
            "Human: What is an alternative phrase for the first thing I said to you?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' You asked me if I had expertise in converting mathematical models to GAMS.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bCLO2VBWqSZj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}